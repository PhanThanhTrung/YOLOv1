{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import xml.etree.ElementTree as ET\n",
    "import matplotlib.image as mimp\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Conv2D,Reshape,LeakyReLU,MaxPooling2D,Dropout,Input, Flatten\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from tensorflow.train import AdamOptimizer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# các param cần thiết cho quá trình training\n",
    "\n",
    "img_w, img_h=640,480\n",
    "grid_w,grid_h=16,12\n",
    "nb_boxes=5\n",
    "grid_size=40\n",
    "width=int(img_w/grid_size)\n",
    "height=int(img_h/grid_size)\n",
    "B=5\n",
    "coord_lamda=5\n",
    "noobj_lamda=0.5\n",
    "classes={'WBC':0,'RBC':1}\n",
    "n_classes=len(classes)\n",
    "anno_path='/Users/hit.flouxetine/datasets/BCCD_Dataset/BCCD/Annotations'\n",
    "image_path='/Users/hit.flouxetine/datasets/BCCD_Dataset/BCCD/JPEGImages'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lấy dữ liệu từ các file xml và decode chúng trành x_train, và y_train.\n",
    "# y_train có hình dạng như sau:(width,heigt,n_classes+5*B)\n",
    "def parse_annotation(anno_path,image_path):\n",
    "    data=[]\n",
    "    for each in sorted(os.listdir(anno_path)):\n",
    "        image={}\n",
    "        tree=ET.parse(anno_path+'/'+each)\n",
    "        Object=[]\n",
    "        for elem in tree.iter():\n",
    "            if 'filename' in elem.tag:\n",
    "                image['filename'] = image_path+'/' + elem.text\n",
    "            if 'width' in elem.tag:\n",
    "                image['width'] = int(elem.text)\n",
    "            if 'height' in elem.tag:\n",
    "                image['height'] = int(elem.text)\n",
    "            if 'object' in elem.tag or 'part' in elem.tag:\n",
    "                obj = {}\n",
    "    \n",
    "                for attr in list(elem):\n",
    "                    if 'name' in attr.tag:\n",
    "                        obj['name'] = attr.text\n",
    "    \n",
    "                    if 'bndbox' in attr.tag:\n",
    "                        for dim in list(attr):\n",
    "                            if 'xmin' in dim.tag:\n",
    "                                obj['xmin'] = int(round(float(dim.text)))\n",
    "                            if 'ymin' in dim.tag:\n",
    "                                obj['ymin'] = int(round(float(dim.text)))\n",
    "                            if 'xmax' in dim.tag:\n",
    "                                obj['xmax'] = int(round(float(dim.text)))\n",
    "                            if 'ymax' in dim.tag:\n",
    "                                obj['ymax'] = int(round(float(dim.text)))\n",
    "                Object.append(obj)\n",
    "        image['object']=Object\n",
    "        \n",
    "        data.append(image)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data():\n",
    "    data=parse_annotation(anno_path, image_path)\n",
    "    N= len(data)\n",
    "    out=[] #y_predict\n",
    "    x=[] #chứa các ảnh\n",
    "    for elem in data:\n",
    "      \n",
    "        img=mimp.imread(elem['filename'])\n",
    "        x.append(img)\n",
    "        truth=np.zeros((height,width,n_classes+B*5))\n",
    "        cnt=np.zeros((height,width)) #to count how many object has been put in cell (i,j)\n",
    "\n",
    "        for obj in elem['object']:\n",
    "            \n",
    "            bbox_x_cen, bbox_y_cen=(obj['xmax']-obj['xmin'])//2+obj['xmin'],(obj['ymax']-obj['ymin'])//2+obj['ymin']\n",
    "            bbox_width,bbox_height= (obj['xmax']-obj['xmin']),(obj['ymax']-obj['ymin'])\n",
    "        \n",
    "        \n",
    "        \n",
    "            #possition of the bbox\n",
    "            pos_x,pos_y=bbox_x_cen//grid_size,bbox_y_cen//grid_size\n",
    "            cnt[pos_y][pos_x]+=1 # increase number of object in cell (i,j) one unit\n",
    "        \n",
    "            if obj['name']=='RBC':\n",
    "                cell_classes=[0,1]\n",
    "            else:\n",
    "                cell_classes=[1,0]\n",
    "        \n",
    "            #normalize bbox\n",
    "            bbox_x_cen,bbox_y_cen=bbox_x_cen/grid_size,bbox_y_cen/grid_size\n",
    "            bbox_width,bbox_height=bbox_width/img_w, bbox_height/img_h\n",
    "            bbox_c_score=1\n",
    "        \n",
    "            #assign value of bbox to the grib cell\n",
    "            if cnt[pos_y][pos_x]==0:\n",
    "                bbox=[bbox_x_cen,bbox_y_cen,bbox_width,bbox_height,bbox_c_score]*B+cell_classes\n",
    "                truth[pos_y][pos_x]=bbox\n",
    "                cnt[pos_y][pos_x]+=1\n",
    "        \n",
    "        out.append(truth)\n",
    "    return [x,out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=encode_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Chuyển x,y vừa tạo được thành numpy array với kiểu dữ liệu float32 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.array(x,dtype='float32')\n",
    "y=np.array(y,dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Viết hàm tính IOU giữa các box. \n",
    "     - Input của hàm IOU sẽ là 2 tensor có hình dạng (batch_size,width,height,B,5)\n",
    "     - Trong đó 5 giá trị của chiều cuối cùng lần lượt là x,y tại vị tri tâm của bounding box, chiều dài, và chiều cao của box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IOU(a,b):\n",
    "    '''\n",
    "        input: two tensor with shape (batch, h, w, number_of_box, 5) represent each box in the grid cell\n",
    "                each box has exactly 5 values which are x_center, y_center, width and height of it.\n",
    "        output:a tensor is iou between those boxes\n",
    "    '''\n",
    "    interval_x=K.maximum(0.0,K.minimum(b[...,0]+b[...,2]/2,a[...,0]+a[...,2]/2)-K.maximum(b[...,0]-b[...,2]/2,a[...,0]-a[...,2]/2))\n",
    "    interval_y=K.maximum(K.minimum(b[...,1]+b[...,3]/2,a[...,1]+a[...,3]/2)-K.maximum(b[...,1]-b[...,3]/2,a[...,1]-a[...,3]/2),0.0)\n",
    "    \n",
    "    bbox_s=a[...,2]*a[...,3]\n",
    "    groundtruth_s=b[...,2]*b[...,3]\n",
    "    intersect=interval_x*interval_y\n",
    "    union=bbox_s+groundtruth_s-intersect\n",
    "    \n",
    "    return intersect/union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Viết hàm custom loss cho yolo.\n",
    "    - đầu vào bao gồm y_true(giá trị thực) và y_predict(giá trị mà model dự đoán ra.\n",
    "    - shape(Batch_size,width,height,B*5+n_classes)\n",
    "    - Lưu ý: Các toạ độ bounding box phải được đưa về toạ độ offset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred):\n",
    "    '''\n",
    "        shape: (batch,height,width,nb_boxes*5+class)\n",
    "    '''\n",
    "    grid=[[float(x),float(y),float(0),float(0),float(0)]*nb_boxes for x in range(grid_h) for y in range(grid_w)]\n",
    "    grid=K.variable(grid)\n",
    "    grid=K.reshape(grid,(1,grid_h,grid_w,nb_boxes,5))\n",
    "    \n",
    "    y_pred_class=y_pred[...,B*5:]\n",
    "    y_true_class=y_true[...,B*5:]\n",
    "    \n",
    "    y_pred_coord=y_pred[...,:B*5]\n",
    "    y_true_coord=y_true[...,:B*5]\n",
    "    \n",
    "    y_pred_coord=K.reshape(y_pred_coord,(-1,grid_h,grid_w,nb_boxes,5))\n",
    "    y_true_coord=K.reshape(y_true_coord,(-1,grid_h,grid_w,nb_boxes,5))\n",
    "    \n",
    "    y_pred_coord=y_pred_coord+grid\n",
    "    \n",
    "    #calculate IOU\n",
    "    iou=IOU(y_true_coord,y_pred_coord)\n",
    "    max_iou=tf.reduce_max(iou,axis=3,keepdims=True)\n",
    "    delta=y_true_coord[...,-1]\n",
    "    object_mask=tf.cast((max_iou<=iou), dtype=tf.float32)*delta\n",
    "    noobject_mask=tf.cast((max_iou>iou), dtype=tf.float32)*delta\n",
    "    \n",
    "    #coordinate loss\n",
    "    \n",
    "    #x_center, y_center\n",
    "    xy_pred_coord=y_pred_coord[...,:2]\n",
    "    xy_true_coord=y_true_coord[...,:2]\n",
    "    xy_score= coord_lamda*K.sum(K.sum(K.square(xy_pred_coord-xy_true_coord),axis=-1)*object_mask)\n",
    "    #w,h coordinate\n",
    "    wh_pred_coord=y_pred_coord[...,2:4]\n",
    "    wh_true_coord=y_true_coord[...,2:4]\n",
    "    wh_score= coord_lamda*K.sum(K.sum(K.square(K.sqrt(wh_pred_coord)-K.sqrt(wh_true_coord)),axis=-1)*object_mask)\n",
    "    \n",
    "    \n",
    "    #confidence score\n",
    "    \n",
    "    c_pred_score=y_pred_coord[...,4]\n",
    "    c_true_score=y_true_coord[...,4]\n",
    "    \n",
    "    c_score= K.sum(K.square(c_true_score*iou-c_pred_score)*object_mask)\n",
    "    c_score+=noobj_lamda*K.sum(K.square(c_true_score*iou-c_pred_score)*noobject_mask)\n",
    "    \n",
    "    \n",
    "    #class loss\n",
    "    cell_mask=y_true[...,-1]\n",
    "    class_score=K.sum(K.sum(K.square(y_true_class-y_pred_class),axis=-1)*cell_mask)\n",
    "    \n",
    "    return xy_score+wh_score+c_score+class_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- khởi tạo model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "model.add(Conv2D(filters=64,kernel_size=(3,3), input_shape=(img_h,img_w,3)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters=64,kernel_size=(3,3)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPooling2D(pool_size=(3,3), strides=2))\n",
    "    \n",
    "model.add(Conv2D(filters=128, kernel_size=(3,3)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters=128,kernel_size=(3,3)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPooling2D(pool_size=(3,3),strides=2))\n",
    "\n",
    "model.add(Conv2D(filters=256, kernel_size=(3,3)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters=256,kernel_size=(3,3)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPooling2D(pool_size=(3,3),strides=2))\n",
    "\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters=512,kernel_size=(3,3)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters=512,kernel_size=(3,3)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPooling2D(pool_size=(3,3),strides=2))\n",
    "\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters=512,kernel_size=(3,3)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters=512,kernel_size=(3,3)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPooling2D(pool_size=(3,3),strides=2))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1088))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dense(1096))\n",
    "model.add(LeakyReLU(0.1))\n",
    "model.add(Dense(units=height*width*(n_classes+5*B), activation='sigmoid'))\n",
    "model.add(Reshape((height,width,(n_classes+5*B))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 478, 638, 64)      1792      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 478, 638, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 476, 636, 64)      36928     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 476, 636, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 237, 317, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 235, 315, 128)     73856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 235, 315, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 233, 313, 128)     147584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 233, 313, 128)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 116, 156, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 114, 154, 256)     295168    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 114, 154, 256)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 112, 152, 256)     590080    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 112, 152, 256)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 55, 75, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 53, 73, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 53, 73, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 51, 71, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 51, 71, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 49, 69, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 49, 69, 512)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 24, 34, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 22, 32, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 22, 32, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 20, 30, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 20, 30, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 18, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 18, 28, 512)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 8, 13, 512)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 53248)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1088)              57934912  \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 1088)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1096)              1193544   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 1096)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5184)              5686848   \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 12, 16, 27)        0         \n",
      "=================================================================\n",
      "Total params: 78,939,912\n",
      "Trainable params: 78,939,912\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = AdamOptimizer(learning_rate=0.001)\n",
    "model.compile(loss=custom_loss, optimizer=adam) # better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sau khi compile model xong thì chúng ta sẽ đi vào training.\n",
    "- số lượng epoch, batch_size sẽ được khai báo bên dưới"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 1\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1220: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Training loss (for one batch) at step 0\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 1\n",
      "Seen so far: 128 samples\n",
      "Training loss (for one batch) at step 2\n",
      "Seen so far: 192 samples\n",
      "Training loss (for one batch) at step 3\n",
      "Seen so far: 256 samples\n",
      "Training loss (for one batch) at step 4\n",
      "Seen so far: 320 samples\n",
      "Training loss (for one batch) at step 5\n",
      "Seen so far: 384 samples\n",
      "Start of epoch 2\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "batch_size = 64\n",
    "N=x.shape[0]\n",
    "for epoch in range(epochs):\n",
    "    print('Start of epoch %d' % (epoch+1,))\n",
    "    for step in range(N//batch_size +1):\n",
    "        index=step*batch_size\n",
    "        last_index=min(index+batch_size,N)\n",
    "        x_batch_train=K.variable(x[index:last_index])\n",
    "        y_batch_train=K.variable(y[index:last_index])\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x_batch_train)\n",
    "            loss_value = custom_loss(y_batch_train, logits)\n",
    "            loss_value += sum(model.losses)\n",
    "            grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        adam.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        \n",
    "        print('Training loss (for one batch) at step %s' % (step))\n",
    "        print('Seen so far: %s samples' % ((step + 1) * 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
